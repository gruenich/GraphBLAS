--------------------------------------------------------------------------------
GxB API changes to GraphBLAS 10.0.0 with the introduction of 32/64 bit indices:
--------------------------------------------------------------------------------

All existing API will work as-is; my matrices will just take less space.  Some 
new API methods will be useful.

A GrB_Matrix of size m-by-n with e entries can have up to 3 different
integer types, uint32_t or uint64_t.  The default will be to use the
smallest integers possible.

    row indices     if m > 2^31, 64-bit integers must be used
    col indices     if n > 2^31, 64-bit integers must be used
    offsets         if e >= 2^32, 64-bit integers must be used

    (I need a single bit in row/col indices for tagging zombies, thus the
    different limits above)

Hypersparse CSR / CSC matrices use all 3 integer types.

CSR / CSC matrices use only two integers (CSR uses col indices, CSC uses row
indices).

Matrices that are in full or bitmap form use no integers, but their hints can
be set/get in case they change formats in the future.

--------------------------------------------------------------------------------
1) GrB_get/set
--------------------------------------------------------------------------------

With GrB_get/set the user application can give a hint as to which integers to
    use, both globally and per-matrix / vector / scalar.  The 3 different
    integers are controlled separately.

    int hint = 32 or 64
    GrB_set (GrB_GLOBAL, hint, GxB_ROWINDEX_INTEGER_HINT)
    GrB_set (GrB_GLOBAL, hint, GxB_COLINDEX_INTEGER_HINT)
    GrB_set (GrB_GLOBAL, hint, GxB_OFFSET_INTEGER_HINT)

    int hint ;  returns 32 or 64
    GrB_get (GrB_GLOBAL, &hint, GxB_ROWINDEX_INTEGER_HINT)
    GrB_get (GrB_GLOBAL, &hint, GxB_COLINDEX_INTEGER_HINT)
    GrB_get (GrB_GLOBAL, &hint, GxB_OFFSET_INTEGER_HINT)

    int hint = 0, 32, or 64, where 0 is the default (which means to use the
    global settings)
    GrB_set (A, hint, GxB_ROWINDEX_INTEGER_HINT)
    GrB_set (A, hint, GxB_COLINDEX_INTEGER_HINT)
    GrB_set (A, hint, GxB_OFFSET_INTEGER_HINT)

    int hint ;  returns 0, 32, or 64
    GrB_get (A, &hint, GxB_ROWINDEX_INTEGER_HINT)
    GrB_get (A, &hint, GxB_COLINDEX_INTEGER_HINT)
    GrB_get (A, &hint, GxB_OFFSET_INTEGER_HINT)

    int bits ;  returns 32 or 64
    GrB_get (A, &bits, GxB_ROWINDEX_INTEGER_BITS)
    GrB_get (A, &bits, GxB_COLINDEX_INTEGER_BITS)
    GrB_get (A, &bits, GxB_OFFSET_INTEGER_BITS)

    A hint of 32 means: "use 32 bits if possible, 64 bits if required"
    A hint of 64 means: "use 64 bits"
    A hint of 0 means: "use the global setting"

    The default global setting will be 32, and the default matrix/vector/scalar
    setting will be 0.  The global hints can be 32 or 64.  The matrix/vector/
    scalar hints can be 0, 32, or 64.  The bits output parameter is always
    32 or 64.

    In the future, this design easily extends to integers of size 8, 16, 128,
    ... bits.

--------------------------------------------------------------------------------
2) changes to GrB*build:
--------------------------------------------------------------------------------

Regardless of the input integer types, I will by default build the matrix
with the smallest integers possible.

I have 14 data types (12 in the spec and 2 GxB for complex single/double).

30 current non-polymorphic:
    GrB_Vector_build_TYPE   (w, uint64_t *I, xtype *X, ...
    GxB_Vector_build_Scalar (w, uint64_t *I, GrB_Scalar x, ...
    GrB_Matrix_build_TYPE   (C, uint64_t *I, uint64_t *J, xtype *X, ...
    GxB_Matrix_build_Scalar (C, uint64_t *I, uint64_t *J, GrB_Scalar x, ...

2 current polymorphic:
    GrB_Vector_build        (w, uint64_t *I, ...
    GrB_Matrix_build        (C, uint64_t *I, uint64_t *J, ...

For 32/64 bit indices, no new polymorphic methods are needed, they are simply
revised to handle different types of I and J.  The question is how many
non-polymorphic methods to add:

first option, I and J have the same integer type:

    30 new non-polymorphic:
    GrB_Vector_build_32_TYPE   (w, uint32_t *I, xtype *X, ...
    GrB_Vector_build_32_Scalar (w, uint32_t *I, GrB_Scalar x, ...
    GrB_Matrix_build_32_TYPE   (C, uint32_t *I, uint32_t *J, xtype *X, ...
    GrB_Vector_build_32_Scalar (C, uint32_t *I, uint32_t *J, GrB_Scalar x, ...

2nd option, I and J can have different integer types:

    75 new non-polymorphic:
    GrB_Vector_build_32_TYPE   (w, uint32_t *I, xtype *X, ...
    GrB_Vector_build_32_Scalar (w, uint32_t *I, GrB_Scalar x, ...

    GrB_Matrix_build_32_32_TYPE   (C, uint32_t *I, uint32_t *J, xtype *X, ...
    GrB_Matrix_build_32_32_Scalar (C, uint32_t *I, uint32_t *J, GrB_Scalar x,...

    GrB_Matrix_build_32_64_TYPE   (C, uint32_t *I, uint64_t *J, xtype *X, ...
    GrB_Matrix_build_32_64_Scalar (C, uint32_t *I, uint64_t *J, GrB_Scalar x,...

    GrB_Matrix_build_64_32_TYPE   (C, uint64_t *I, uint32_t *J, xtype *X, ...
    GrB_Matrix_build_64_32_Scalar (C, uint64_t *I, uint32_t *J, GrB_Scalar x,...

    GrB_Matrix_build_64_64_TYPE   (C, uint64_t *I, uint64_t *J, xtype *X, ...
    GrB_Matrix_build_64_64_Scalar (C, uint64_t *I, uint64_t *J, GrB_Scalar x,...

Extra option: in addition to the above, I will add 4 new methods, as yet
    unnamed, where all C arrays are replaced by GrB_Vectors:

    GxB_Vector_build_? (w, GrB_Vector I, GrB_Vector X, dup, desc)
    GxB_Matrix_build_? (C, GrB_Vector I, GrB_Vector J, GrB_Vector X, dup, desc)

    GxB_Vector_build_? (w, GrB_Vector I, GrB_Scalar X, desc)
    GxB_Matrix_build_? (C, GrB_Vector I, GrB_Vector J, GrB_Scalar X, desc)

    All vector inputs (I, J, X) must have the same number of entries.  They
    may be sparse.  The descriptor will have 2 options each for each I,J,X:
    either use the indices of the given vector, or its values.  If used, the
    values must be of type int32_t, uint32_t, int64_t, or uint64_t.

    If X is a GrB_Scalar, then the matrix/vector output will be iso-valued,
    with all entries in the matrix/vector equal to the scalar X.  There is
    no dup operator.

    Even without adding support for 32-bit integers, these all-vector methods
    are important; many LAGraph algorithms use GraphBLAS to compute the index
    arrays I and J, as GrB_Vectors.  They must then be extractTuple'd or
    unpacked to get uint64_t * C arrays, just to pass them back to GrB*_build,
    GrB_assign, GrB_extract, and so on.  Argh!

Pros of 2nd option: gives more flexibility to the user.  My internal codes
    already allow any arbitrary mix.  This would be easy to add.

Cons of 2nd option: cause a massive combinatorial explosion of
    nonpolymorphic methods even with just 2 kinds of integers.  What if we
    add support for 8, 16, 128, 256, ... bit integers in the future??

    16 bits would be be handy for a vector database: consider a CSR matrix of
    size m-by-n with e entries, where m = 10 million and n < 64 thousand, and
    an average of 100 entries per row.  Offset integers are 32 bit, and col
    indices are just 16 bit.  Space is cut almost by 4x compared with
    all-64-bit integers.

3rd option: Extra option only, just add 4 new methods.
    Pros: very simple, very flexible.
    To build with 32-bit integers, I and J would first need to be
    "packed" using GxB_pack, converting uint32_t *I to a GrB_Vector I, in
    O(1) time and space.
    Cons: GxB pack/unpack is not in the spec.

--------------------------------------------------------------------------------
3) changes to GrB*extractTuples
--------------------------------------------------------------------------------

Similar to GrB*build.  We do need to add the ability to extract any subset of
I,J,X to the standard GrB C API.  SuiteSparse allows these C arrays to be
passed in as NULL, which is not an error.  It just means "don't extract those".
For example, to get the pattern of A in the integer arrays I and J, but not
extracting the values:

    GrB_Matrix_extractTuples_FP32 (I, J, NULL, &nvals, A)

We could consider adding 2 vector-based methods:

GrB_Matrix_extractTuples_name (GrB_Vector I, GrB_Vector J, GrB_Vector X, A)
GrB_Vector_extractTuples_name (GrB_Vector I, GrB_Vector X, A)

to extract the tuples as dense GrB_Vectors, of any integer data type desired.
This design extends to arbitrary integer types in the future.

--------------------------------------------------------------------------------
3) changes to GrB*assign and GrB*extract
--------------------------------------------------------------------------------

Similar to GrB*build, including the variants where I and J are GrB_Vectors,
not pointers to C arrays.

--------------------------------------------------------------------------------
4) changes to GxB*import / export
--------------------------------------------------------------------------------

This could be left as-is.  import/export is costly, and as a result, LAGraph
never uses it if SuiteSparse is available.  It uses pack/unpack instead.

However, we could extend import/export as we do GrB*extractTuples.

For exisiting methods, I will just convert the integers on import/export, with
no added cost except useless extra space on the user side.

--------------------------------------------------------------------------------
5) changes to GxB*pack / unpack
--------------------------------------------------------------------------------

The simplest solution will be to add just 4 methods: A new pack/unpack for
GrB_Vectors and a new pack/unpack for GrB_Matrices.  They will have NO
parameters that are pointers to C arrays.  Instead they will pack/unpack into
dense GrB_Vectors.  The methods will work for all of my data formats, and any
integer sizes (32 or 64 bit, and in the future, any bit size you like), with no
need to add new methods if new integer sizes are supported in the future.

    GxB_Matrix_pack (GrB_Matrix A,
        GrB_Vector Ap,     // offsets for CSR, CSC, HyperCSR, HyperCSC
        GrB_Vector Ah,     // hyperlist for HyperCSR, HyperCSC
        GrB_Vector Ai,     // row/col indices for CSR, CSC, HyperCSR, HyperCSC
        GrB_Vector Ab,     // int8_t for bitmap only
        GrB_Vector Ax,     // values for all formats
        GrB_Matrix Ay,     // optional hyperhash for HyperCSR/HyperCSC
        bool jumbled,      // true if indices in Ai may be out of order
        int format,        // 8 cases: CSR, CSC, hyperCSR, HyperCSC,
                           //       bitmap by row, bitmap by col
                           //       full by row, full by col
        descriptor) ;

On input, those 6 vectors must all be dense or some may be NULL.  To pack a CSR
matrix, Ap must be a dense integer vector of size nrows(A)+1, Ai must be a
dense integer vector of size nvals(A), and so on.  Ax must be a dense vector of
size nvals(A), or size 1 if A is iso-valued. The inputs Ah, Ab, and Ay must be
NULL, or have no entries, to indicate that the matrix A will be created as CSR.

    GxB_Matrix_unpack (GrB_Matrix A,
        GrB_Vector Ap,     // offsets for CSR, CSC, HyperCSR, HyperCSC
        GrB_Vector Ah,     // hyperlist for HyperCSR, HyperCSC
        GrB_Vector Ai,     // row/col indices for CSR, CSC, HyperCSR, HyperCSC
        GrB_Vector Ab,     // int8_t for bitmap only
        GrB_Vector Ax,     // values for all formats
        GrB_Matrix Ay,     // optional hyperhash for HyperCSR/HyperCSC
        bool *jumbled,     // true if indices in Ai may be out of order
        int *format,
        descriptor) ;

For vectors:

    GxB_Vector_pack (GrB_Vector V,
        GrB_Vector Vp,     // offsets for CSR, CSC (at most size 2)
        GrB_Vector Vi,     // row/col indices for CSR, CSC
        GrB_Vector Vb,     // int8_t for bitmap only
        GrB_Vector Vx,     // values
        bool jumbled,      // true if indices in Vi may be out of order
        int format,
        descriptor) ;

    GxB_Vector_unpack (GrB_Vector V,
        GrB_Vector Vp,     // offsets for CSR, CSC
        GrB_Vector Vi,     // row/col indices for CSR, CSC
        GrB_Vector Vb,     // int8_t for bitmap only
        GrB_Vector Vx,     // values
        bool *jumbled,     // true if indices in Vi may be out of order
        int *format,
        descriptor) ;

These methods would be guaranteed to take O(1) time and space since they will
not do any data format changes.

Then if the user application needs any of this data unpacked into C arrays,
just use my existing GxB_Vector_unpack_Full to get the C array and its size, in
O(1) time and space.  Likewise, to pack from C arrays, just use
GxB_Vector_pack_Full to create each input to the new pack methods above.

For backward compatibility, I will keep all my existing pack/unpack methods.
If a GrB_Matrix is unpacked into 64-bit arrays, but the data is internally
stored in 32-bit integers, I will just have to convert them first.  My
exising pack/unpack methods do not guarantee O(1) time/space.  For
example, if GxB_Matrix_unpack_CSR is called and the matrix is already CSR,
it takes O(1) time/space.  Otherwise, I must convert it to CSR (taking lots
of time/space) and then unpack it to the requested CSR format.

I haven't written these 4 methods yet, but they will simple to do. Only the
API design needs some more thought.

The only downside to these methods is that they assume only 8 data formats:
CSR, CSC, HyperCSR, HyperCSC, bitmap by row, bitmap by col, full by row,
and full by column.  If I add new data formats in the future, I may have to
extend these APIs.

Alternatively, I could use arrays of GrB_Vectors for the parameters.
A future-safe variant might look like this:

    GxB_Matrix_pack (GrB_Matrix A,
        GrB_Vector Stuff [...],         // for Ap, Ah, Ai, Ab, Ax
        GrB_Matrix MoreStuff [...],     // for Ay
        int properties [...]    // many cases, 8 now and more in the future,
                                // jumbled true/false, etc
        descriptor) ;

That way I could add many kinds of future data structures without changing
the API.

Any thoughts?  Lots to talk about in the Jan 8th LAGraph+SuiteSparse:GraphBLAS
meeting.  If you can't make it, please email any feedback, or we can meet
another time.

